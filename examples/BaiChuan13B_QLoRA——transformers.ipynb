{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7bef94-e459-4140-b533-cd6c188722b0",
   "metadata": {},
   "source": [
    "# Baichuan-13B ä¿å§†çº§å¾®è°ƒèŒƒä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf35ed-b6c1-477f-89bb-c059573bffcf",
   "metadata": {},
   "source": [
    "ğŸ˜‹ğŸ˜‹å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®è¯ï¼š**torchkeras**ï¼Œè·å–æœ¬æ–‡notebookæºä»£ç å’Œæ•°æ®é›†ä¸‹è½½é“¾æ¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbccebf-138c-445d-b386-9e4add0ad3ed",
   "metadata": {},
   "source": [
    "å¹²è´§é¢„è­¦ï¼šè¿™å¯èƒ½æ˜¯ä½ èƒ½å¤Ÿæ‰¾åˆ°çš„æœ€å®¹æ˜“æ‡‚çš„ï¼Œæœ€å®Œæ•´çš„ï¼Œé€‚ç”¨äºå„ç§NLPä»»åŠ¡çš„Baichuan-13B-Chatçš„finetuneæ•™ç¨‹~\n",
    "\n",
    "Baichuan-13Bæ˜¯ç™¾å·æ™ºèƒ½äº2023å¹´7æœˆ11æ—¥å‘å¸ƒçš„å¼€æºä¸­è‹±åŒè¯­LLMï¼Œå„é¡¹æŒ‡æ ‡ç»è¯„æµ‹åœ¨å¼€æºLLMä¸­åŒå°ºå¯¸æ¨¡å‹ä¸­ä½å±…å‰åˆ—ã€‚\n",
    "\n",
    "Baichuan-13BåŒ…æ‹¬Baichuan-13B-Baseå’ŒBaichuan-13B-chatä¸¤ä¸ªä¸åŒæ¨¡å‹ã€‚å‰è€…ä»…ä»…æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œåè€…åœ¨å‰è€…åŸºç¡€ä¸Šå¢åŠ äº†SFT,RLHFç­‰åå¥½å¯¹é½è¿‡ç¨‹ã€‚\n",
    "\n",
    "æœ¬èŒƒä¾‹å¾®è°ƒçš„æ¨¡å‹æ˜¯Baichuan-13B-Chatï¼Œæˆ‘ä»¬ä½¿ç”¨éå¸¸ç®€å•çš„ï¼Œå¤–å–è¯„è®ºæ•°æ®é›†æ¥å®æ–½å¾®è°ƒï¼Œå¯¹ä¸€æ®µå¤–å–è¯„è®ºåŒºåˆ†æ˜¯å¥½è¯„è¿˜æ˜¯å·®è¯„ã€‚\n",
    "\n",
    "å¯ä»¥å‘ç°ï¼Œç»è¿‡å¾®è°ƒåçš„æ¨¡å‹ï¼Œç›¸æ¯”ç›´æ¥ 3-shot-prompt å¯ä»¥å–å¾—æ˜æ˜¾æ›´å¥½çš„æ•ˆæœ(0.89->0.90)ã€‚\n",
    "\n",
    "è™½ç„¶Baichuan-13B-Chatæ˜¯ä¸€ä¸ªç™¾äº¿çº§çš„LLMï¼Œä½†ç”±äºæˆ‘ä»¬ä½¿ç”¨éå¸¸èŠ‚çº¦æ˜¾å­˜çš„QLoRAå¾®è°ƒç®—æ³•ï¼Œå…·å¤‡32Gå·¦å³æ˜¾å­˜çš„GPUå³å¯å®æ–½æœ¬è¿‡ç¨‹ã€‚\n",
    "\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬ä»¥æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œå®é™…ä¸Šï¼Œä»»ä½•NLPä»»åŠ¡ï¼Œä¾‹å¦‚ï¼Œå‘½åå®ä½“è¯†åˆ«ï¼Œç¿»è¯‘ï¼ŒèŠå¤©å¯¹è¯ç­‰ç­‰ï¼Œéƒ½å¯ä»¥é€šè¿‡åŠ ä¸Šåˆé€‚çš„ä¸Šä¸‹æ–‡ï¼Œè½¬æ¢æˆä¸€ä¸ªå¯¹è¯é—®é¢˜ï¼Œå¹¶é’ˆå¯¹æˆ‘ä»¬çš„ä½¿ç”¨åœºæ™¯ï¼Œè®¾è®¡å‡ºåˆé€‚çš„æ•°æ®é›†æ¥å¾®è°ƒBaichuan-13B-Chat.\n",
    "\n",
    "\n",
    "æ³¨ï¼Œæœ¬æ•™ç¨‹æ˜¯ ChatGLM2-6bä¿å§†çº§å¾®è°ƒèŒƒä¾‹ çš„å…„å¼Ÿç‰ˆæœ¬~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee10aa6-bd22-4014-8fa4-7f5a49c884b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T08:53:04.969095751Z",
     "start_time": "2023-12-03T08:52:42.861416266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bitsandbytes 0.39.1\r\n",
      "Uninstalling bitsandbytes-0.39.1:\r\n",
      "  Would remove:\r\n",
      "    /usr/local/lib/python3.10/dist-packages/bitsandbytes-0.39.1.dist-info/*\r\n",
      "    /usr/local/lib/python3.10/dist-packages/bitsandbytes/*\r\n",
      "Proceed (Y/n)? ^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "#å®‰è£…ç¯å¢ƒ\n",
    "\n",
    "# #baichuan-13b-chat\n",
    "# !pip install 'transformers==4.30.2'\n",
    "# !pip install  -U transformers_stream_generator\n",
    "# \n",
    "# \n",
    "# #finetune\n",
    "# !pip install datasets\n",
    "# !pip install git+https://github.com/huggingface/accelerate\n",
    "# !pip install  git+https://github.com/huggingface/peft\n",
    "# !pip install  git+https://github.com/lyhue1991/torchkeras \n",
    "!pip install 'bitsandbytes' #4bité‡åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237a11c4-0f74-4f67-9971-684cd0903342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:02.758825990Z",
     "start_time": "2023-12-03T14:13:02.684770354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29G\t/home/zhongbing/Projects/MLE/models/Baichuan2-7B-Chat\r\n"
     ]
    }
   ],
   "source": [
    "!du -s -h ~/Projects/MLE/models/Baichuan2-7B-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3a0b3-bf03-46bd-bace-330665645f9a",
   "metadata": {},
   "source": [
    "## ã€‡ï¼Œé¢„è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e633e3b-e4cc-48d4-8792-ea269c7396f6",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬éœ€è¦ä» https://huggingface.co/baichuan-inc/Baichuan-13B-Chat ä¸‹è½½baichuan-13b-chatçš„æ¨¡å‹ã€‚\n",
    "\n",
    "å›½å†…å¯èƒ½é€Ÿåº¦ä¼šæ¯”è¾ƒæ…¢ï¼Œæ€»å…±æœ‰25ä¸ªGå·¦å³ï¼Œç½‘é€Ÿä¸å¤ªå¥½çš„è¯ï¼Œå¤§æ¦‚å¯èƒ½éœ€è¦ä¸¤åˆ°ä¸‰ä¸ªå°æ—¶ã€‚\n",
    "\n",
    "å¦‚æœç½‘ç»œä¸ç¨³å®šï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨ä»è¿™ä¸ªé¡µé¢ä¸€ä¸ªä¸€ä¸ªä¸‹è½½å…¨éƒ¨æ–‡ä»¶ç„¶åæ”¾ç½®åˆ° ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ä¾‹å¦‚ 'baichuan-13b' ä»¥ä¾¿è¯»å–ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0f2e17-7ed1-42e8-81f8-63a7bbeab31e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:10.074095885Z",
     "start_time": "2023-12-03T14:13:10.058442673Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88bf5d19eabd4d728d3db3b80eae3e34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, AutoModel, BitsAndBytesConfig\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#ä½¿ç”¨QLoRAå¼•å…¥çš„ NF4é‡åŒ–æ•°æ®ç±»å‹ä»¥èŠ‚çº¦æ˜¾å­˜\n",
    "model_name_or_path ='/home/zhongbing/Projects/MLE/models/Baichuan2-7B-Chat' #è¿œç¨‹ 'baichuan-inc/Baichuan-13B-Chat'\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "   model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True) \n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:22.625241491Z",
     "start_time": "2023-12-03T14:13:11.200830168Z"
    }
   },
   "id": "e200470d-5cec-44fd-a733-b26b7ff43a63"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76722209-62a4-4e17-a672-9383791cd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±å³°æ˜¯å–œé©¬æ‹‰é›…å±±è„‰çš„ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰ï¼Œæµ·æ‹”é«˜åº¦ä¸º8,848ç±³ï¼ˆ29,031è‹±å°ºï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output \n",
    "messages = []\n",
    "messages.append({\"role\": \"user\",\n",
    "                 \"content\": \"ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±å³°æ˜¯å“ªåº§?\"})\n",
    "response = model.chat(tokenizer,messages=messages,stream=True)\n",
    "for res in response:\n",
    "    print(res)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cd14e-35c2-4e15-bf5f-1b757aefc1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4045ef-14c7-4eb3-b577-a70e81d23ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c754a8e9-c9d6-49c4-8a43-daafc8374129",
   "metadata": {},
   "source": [
    "ä¸‹é¢æˆ‘ä»¬è®¾è®¡ä¸€ä¸ª3-shot-promptæ–¹æ³•ï¼Œä½¿ç”¨å¤–å–æ•°æ®é›†æµ‹è¯•ä¸€ä¸‹BaiChuan13bçš„æ–‡æœ¬åˆ†ç±»èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c1e15f-2217-4a76-8caa-945362f98317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:28.049256356Z",
     "start_time": "2023-12-03T14:13:28.042823391Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\"ä½ æ˜¯ä¸€ä¸ªè¦ç´ æå–ä¸“å®¶ï¼Œä½ éœ€è¦ä»è¾“å…¥æ–‡æœ¬ä¸­æå–è´­æ±‡æˆ–ç»“æ±‡ä¿¡æ¯,å…¶ä¸­è´­æ±‡æ˜¯ä¹°å…¥å¤–æ±‡ï¼Œç»“æ±‡æ˜¯å–å‡ºå¤–æ±‡ã€‚å¦‚æœéå†è¾“å…¥æ–‡æœ¬æ²¡æœ‰æ˜ç¡®æåˆ°è´­æ±‡æˆ–ç»“æ±‡ï¼Œåˆ™è¿”å›nanã€‚\n",
    "ä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹ï¼Œæ¡ˆä¾‹æ— éœ€å¤„ç†:\n",
    "\n",
    "```\n",
    "è¾“å…¥ï¼šæ˜å¤©ç¾å…ƒè´­æ±‡\n",
    "è¾“å‡ºï¼šè´­æ±‡\n",
    "\n",
    "è¾“å…¥ï¼šæ¬§å…ƒç»“æ±‡\n",
    "è¾“å‡ºï¼šç»“æ±‡\n",
    "\n",
    "è¾“å…¥ï¼šæ¬§å…ƒè´­æ±‡\n",
    "è¾“å‡ºï¼šè´­æ±‡\n",
    "\n",
    "è¾“å…¥ï¼šå–ç¾å…ƒä¹°ç¾å…ƒ\n",
    "è¾“å‡ºï¼šnan\n",
    "```\n",
    "\n",
    "ä»¥ä¸‹ä¸ºä¸€ä¸ªçœŸå®è¾“å…¥ï¼Œåªéœ€è¾“å‡ºç»“æ±‡ã€è´­æ±‡æˆ–nanï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ— å…³å†…å®¹ï¼š\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(text):\n",
    "    return prefix+text+'\\nè¾“å‡ºï¼š'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9974c379-3bf3-4089-b656-3d0861b1e76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:29.458017451Z",
     "start_time": "2023-12-03T14:13:29.220079770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»“æ±‡\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'role': 'user',\n  'content': 'ä½ æ˜¯ä¸€ä¸ªè¦ç´ æå–ä¸“å®¶ï¼Œä½ éœ€è¦ä»è¾“å…¥æ–‡æœ¬ä¸­æå–è´­æ±‡æˆ–ç»“æ±‡ä¿¡æ¯,å…¶ä¸­è´­æ±‡æ˜¯ä¹°å…¥å¤–æ±‡ï¼Œç»“æ±‡æ˜¯å–å‡ºå¤–æ±‡ã€‚å¦‚æœéå†è¾“å…¥æ–‡æœ¬æ²¡æœ‰æ˜ç¡®æåˆ°è´­æ±‡æˆ–ç»“æ±‡ï¼Œåˆ™è¿”å›nanã€‚\\nä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹ï¼Œæ¡ˆä¾‹æ— éœ€å¤„ç†:\\n\\n```\\nè¾“å…¥ï¼šæ˜å¤©ç¾å…ƒè´­æ±‡\\nè¾“å‡ºï¼šè´­æ±‡\\n\\nè¾“å…¥ï¼šæ¬§å…ƒç»“æ±‡\\nè¾“å‡ºï¼šç»“æ±‡\\n\\nè¾“å…¥ï¼šæ¬§å…ƒè´­æ±‡\\nè¾“å‡ºï¼šè´­æ±‡\\n\\nè¾“å…¥ï¼šå–ç¾å…ƒä¹°ç¾å…ƒ\\nè¾“å‡ºï¼šnan\\n```\\n\\nä»¥ä¸‹ä¸ºä¸€ä¸ªçœŸå®è¾“å…¥ï¼Œåªéœ€è¾“å‡ºç»“æ±‡ã€è´­æ±‡æˆ–nanï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ— å…³å†…å®¹ï¼š\\nä¹°å…¥ç¾å…ƒå–å‡ºäººæ°‘å¸\\nè¾“å‡ºï¼š'}]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages  = [{\"role\": \"user\", \"content\": get_prompt('ä¹°å…¥ç¾å…ƒå–å‡ºäººæ°‘å¸')}]\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5ea5e0-70a0-4e72-8452-485819157bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T11:26:54.582406123Z",
     "start_time": "2023-12-03T11:26:54.573924861Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c7d5d3-4ec0-47f4-8592-6cc77e7d54a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:32.861723242Z",
     "start_time": "2023-12-03T14:13:32.815484723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'ä½ æ˜¯ä¸€ä¸ªè¦ç´ æå–ä¸“å®¶ï¼Œä½ éœ€è¦ä»è¾“å…¥æ–‡æœ¬ä¸­æå–è´­æ±‡æˆ–ç»“æ±‡ä¿¡æ¯,å…¶ä¸­è´­æ±‡æ˜¯ä¹°å…¥å¤–æ±‡ï¼Œç»“æ±‡æ˜¯å–å‡ºå¤–æ±‡ã€‚å¦‚æœéå†è¾“å…¥æ–‡æœ¬æ²¡æœ‰æ˜ç¡®æåˆ°è´­æ±‡æˆ–ç»“æ±‡ï¼Œåˆ™è¿”å›nanã€‚\\nä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹ï¼Œæ¡ˆä¾‹æ— éœ€å¤„ç†:\\n\\n```\\nè¾“å…¥ï¼šæ˜å¤©ç¾å…ƒè´­æ±‡\\nè¾“å‡ºï¼šè´­æ±‡\\n\\nè¾“å…¥ï¼šæ¬§å…ƒç»“æ±‡\\nè¾“å‡ºï¼šç»“æ±‡\\n\\nè¾“å…¥ï¼šæ¬§å…ƒè´­æ±‡\\nè¾“å‡ºï¼šè´­æ±‡\\n\\nè¾“å…¥ï¼šå–ç¾å…ƒä¹°ç¾å…ƒ\\nè¾“å‡ºï¼šnan\\n```\\n\\nä»¥ä¸‹ä¸ºä¸€ä¸ªçœŸå®è¾“å…¥ï¼Œåªéœ€è¾“å‡ºç»“æ±‡ã€è´­æ±‡æˆ–nanï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ— å…³å†…å®¹ï¼š\\nä¹°å…¥ç¾å…ƒå–å‡ºäººæ°‘å¸\\nè¾“å‡ºï¼š'}, {'role': 'assistant', 'content': 'ç»“æ±‡'}]\n"
     ]
    }
   ],
   "source": [
    "messages = messages+[{\"role\": \"assistant\", \"content\": response}]\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a078a521-b3c3-40c8-b7a5-9505a9cbc80e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:34.992431153Z",
     "start_time": "2023-12-03T14:13:34.977062131Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_message(prompt,response):\n",
    "    return [{\"role\": \"user\", \"content\": f'{prompt} -> '},\n",
    "            {\"role\": \"assistant\", \"content\": response}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30247e96-0f93-4c05-bc2d-8c836db05dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:35.818911754Z",
     "start_time": "2023-12-03T14:13:35.812910440Z"
    }
   },
   "outputs": [],
   "source": [
    "messages.extend(get_message('20230909è´­æ±‡æ¬§å…ƒ','è´­æ±‡'))\n",
    "messages.extend(get_message('éæ—¥å…ƒç»“æ±‡','ç»“æ±‡'))\n",
    "messages.extend(get_message('è¿™ä¹ˆå’¸ï¼ŒçœŸçš„æ˜¯é†‰äº†','nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e1c5fdc-afd9-44d9-a96b-8591af55801c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:37.061233302Z",
     "start_time": "2023-12-03T14:13:37.048568861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'role': 'user',\n  'content': 'ä½ æ˜¯ä¸€ä¸ªè¦ç´ æå–ä¸“å®¶ï¼Œä½ éœ€è¦ä»è¾“å…¥æ–‡æœ¬ä¸­æå–è´­æ±‡æˆ–ç»“æ±‡ä¿¡æ¯,å…¶ä¸­è´­æ±‡æ˜¯ä¹°å…¥å¤–æ±‡ï¼Œç»“æ±‡æ˜¯å–å‡ºå¤–æ±‡ã€‚å¦‚æœéå†è¾“å…¥æ–‡æœ¬æ²¡æœ‰æ˜ç¡®æåˆ°è´­æ±‡æˆ–ç»“æ±‡ï¼Œåˆ™è¿”å›nanã€‚\\nä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹ï¼Œæ¡ˆä¾‹æ— éœ€å¤„ç†:\\n\\n```\\nè¾“å…¥ï¼šæ˜å¤©ç¾å…ƒè´­æ±‡\\nè¾“å‡ºï¼šè´­æ±‡\\n\\nè¾“å…¥ï¼šæ¬§å…ƒç»“æ±‡\\nè¾“å‡ºï¼šç»“æ±‡\\n\\nè¾“å…¥ï¼šæ¬§å…ƒè´­æ±‡\\nè¾“å‡ºï¼šè´­æ±‡\\n\\nè¾“å…¥ï¼šå–ç¾å…ƒä¹°ç¾å…ƒ\\nè¾“å‡ºï¼šnan\\n```\\n\\nä»¥ä¸‹ä¸ºä¸€ä¸ªçœŸå®è¾“å…¥ï¼Œåªéœ€è¾“å‡ºç»“æ±‡ã€è´­æ±‡æˆ–nanï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ— å…³å†…å®¹ï¼š\\nä¹°å…¥ç¾å…ƒå–å‡ºäººæ°‘å¸\\nè¾“å‡ºï¼š'},\n {'role': 'assistant', 'content': 'ç»“æ±‡'},\n {'role': 'user', 'content': '20230909è´­æ±‡æ¬§å…ƒ -> '},\n {'role': 'assistant', 'content': 'è´­æ±‡'},\n {'role': 'user', 'content': 'éæ—¥å…ƒç»“æ±‡ -> '},\n {'role': 'assistant', 'content': 'ç»“æ±‡'},\n {'role': 'user', 'content': 'è¿™ä¹ˆå’¸ï¼ŒçœŸçš„æ˜¯é†‰äº† -> '},\n {'role': 'assistant', 'content': 'nan'}]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdaf6e0f-fd17-40e1-b790-78593e382d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:38.129936305Z",
     "start_time": "2023-12-03T14:13:38.123390025Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text,temperature=0.01):\n",
    "    model.generation_config.temperature=temperature\n",
    "    response = model.chat(tokenizer, \n",
    "                          messages = messages+[{'role':'user','content':f'{text} -> '}])\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2dffa85-2932-4bee-b60d-965eedd88604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:13:39.435069334Z",
     "start_time": "2023-12-03T14:13:39.196853220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'è´­æ±‡'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('æˆ‘æƒ³å–ä¸‹ä¸ªæœˆçš„ç¾å…ƒ') #æƒŠå–œï¼Œå±…ç„¶é¢„æµ‹å¯¹äº†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f9d62-34bb-4589-9bb5-f25090199775",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ‹¿å¤–å–æ•°æ®é›†æ¥æµ‹è¯•ä¸€ä¸‹æœªç»å¾®è°ƒï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cadf4887-a715-4e36-9e80-419619f4b983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:34:09.260367382Z",
     "start_time": "2023-12-03T14:34:09.215719817Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datasets \n",
    "from tqdm import tqdm \n",
    "\n",
    "#æ•°æ®é›†åˆ†å‰²\n",
    "df = pd.read_csv(\"./ZXC_Forward_Tuning_Cases.csv\")\n",
    "\n",
    "# replace all except 'è´­æ±‡' and 'ç»“æ±‡' with 'nan' in a df\n",
    "df['tag'] = df['è´­æ±‡ç»“æ±‡'].apply(lambda x: x if x in ['è´­æ±‡','ç»“æ±‡'] else 'nan')\n",
    "\n",
    "# calculate the number of all candidate for tag\n",
    "df['tag'].value_counts()\n",
    "\n",
    "# df = df.rename({'review':'text'},axis = 1)\n",
    "# \n",
    "# dfgood = df.query('tag==\"å¥½è¯„\"')\n",
    "# dfbad = df.query('tag==\"å·®è¯„\"').head(len(dfgood)) #é‡‡æ ·éƒ¨åˆ†å·®è¯„ï¼Œè®©å¥½è¯„å·®è¯„å¹³è¡¡\n",
    "# df = pd.concat([dfgood,dfbad])\n",
    "# \n",
    "ds_dic = datasets.Dataset.from_pandas(df).train_test_split(\n",
    "    test_size = 10,shuffle=True, seed = 43)\n",
    "dftrain = ds_dic['train'].to_pandas()\n",
    "dftest = ds_dic['test'].to_pandas()\n",
    "dftrain.to_parquet('./data/dftrain.parquet')\n",
    "dftest.to_parquet('./data/dftest.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f41ca74-cb57-4f4d-a74a-66ad39262224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:34:13.406867807Z",
     "start_time": "2023-12-03T14:34:13.393271708Z"
    }
   },
   "outputs": [],
   "source": [
    "#æ•°æ®é›†åŠ è½½\n",
    "dftrain = pd.read_parquet('./data/dftrain.parquet')[['INPUT','è´­æ±‡ç»“æ±‡','tag']]\n",
    "dftest = pd.read_parquet('./data/dftest.parquet')[['INPUT','è´­æ±‡ç»“æ±‡','tag']]\n",
    "ds_train,ds_val = datasets.Dataset.from_pandas(dftrain).train_test_split(\n",
    "    test_size=10,seed=42).values()\\\n",
    "\n",
    "dftrain,dfval = ds_train.to_pandas(), ds_val.to_pandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44fe3c7a-2857-478c-9041-42713e4b5b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:34:28.349533016Z",
     "start_time": "2023-12-03T14:34:25.883986966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dftest['pred'] = [predict(text) for text in tqdm(dftest['INPUT'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82b2d2ab-ef2e-47c3-b7de-5b1a1c32877a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:35:08.880124384Z",
     "start_time": "2023-12-03T14:35:08.872031333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                 INPUT  è´­æ±‡ç»“æ±‡  tag pred\n0   EURCNY 20221111 ç»“æ±‡    ç»“æ±‡   ç»“æ±‡   ç»“æ±‡\n1  @æ‹›å°èƒ sell usdcny 3m  None  nan   ç»“æ±‡\n2                 å³æœŸç»“æ±‡    ç»“æ±‡   ç»“æ±‡   ç»“æ±‡\n3       .USDCNH T+0 ç»“æ±‡    ç»“æ±‡   ç»“æ±‡   ç»“æ±‡\n4      USDCNH 20231215  None  nan   è´­æ±‡\n5      SELL EURCNY T+0  None  nan   ç»“æ±‡\n6       Sell USDCNY åå¤©  None  nan   ç»“æ±‡\n7   USDeur 20231016 è´­æ±‡    è´­æ±‡   è´­æ±‡   è´­æ±‡\n8           ä»Šæ—¥ç¾å…ƒå…‘äººæ°‘å¸ä»·æ ¼  None  nan  nan\n9       è¿œæœŸç¾å…ƒè´­æ±‡20230710    è´­æ±‡   è´­æ±‡   è´­æ±‡",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>INPUT</th>\n      <th>è´­æ±‡ç»“æ±‡</th>\n      <th>tag</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EURCNY 20221111 ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@æ‹›å°èƒ sell usdcny 3m</td>\n      <td>None</td>\n      <td>nan</td>\n      <td>ç»“æ±‡</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>å³æœŸç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>.USDCNH T+0 ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n      <td>ç»“æ±‡</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USDCNH 20231215</td>\n      <td>None</td>\n      <td>nan</td>\n      <td>è´­æ±‡</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SELL EURCNY T+0</td>\n      <td>None</td>\n      <td>nan</td>\n      <td>ç»“æ±‡</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Sell USDCNY åå¤©</td>\n      <td>None</td>\n      <td>nan</td>\n      <td>ç»“æ±‡</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>USDeur 20231016 è´­æ±‡</td>\n      <td>è´­æ±‡</td>\n      <td>è´­æ±‡</td>\n      <td>è´­æ±‡</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ä»Šæ—¥ç¾å…ƒå…‘äººæ°‘å¸ä»·æ ¼</td>\n      <td>None</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>è¿œæœŸç¾å…ƒè´­æ±‡20230710</td>\n      <td>è´­æ±‡</td>\n      <td>è´­æ±‡</td>\n      <td>è´­æ±‡</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.query('tag==pred').shape  \n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dfb5ddd-8573-4c38-b746-68adea5aeea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:35:31.572850462Z",
     "start_time": "2023-12-03T14:35:31.560005137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc =  0.6\n"
     ]
    }
   ],
   "source": [
    "print('acc = ',len(dftest.query('tag==pred'))/len(dftest))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7015a6a2-451b-4db8-9c22-31f24bb68ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:36:05.436270461Z",
     "start_time": "2023-12-03T14:36:05.394064324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tag   nan   ç»“æ±‡   è´­æ±‡\npred               \nnan   1.0  NaN  NaN\nç»“æ±‡    3.0  3.0  NaN\nè´­æ±‡    1.0  NaN  2.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>tag</th>\n      <th>nan</th>\n      <th>ç»“æ±‡</th>\n      <th>è´­æ±‡</th>\n    </tr>\n    <tr>\n      <th>pred</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>nan</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ç»“æ±‡</th>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>è´­æ±‡</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.pivot_table(index='pred',columns = 'tag',\n",
    "                   values='INPUT',aggfunc='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083eecf-6bd8-4450-9413-d10742eaffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09be19b6-615f-4488-bd27-6dc08a015105",
   "metadata": {},
   "source": [
    "## ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d48441-441e-41c0-9442-7aeb8da003c3",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä»¿ç…§ç™¾å·æ¨¡å‹çš„ `model._build_chat_input` æ–¹æ³•æ¥è¿›è¡Œtokenç¼–ç ï¼ŒåŒæ—¶æŠŠéœ€è¦å­¦ä¹ çš„å†…å®¹æ·»åŠ label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d442edb-dcd5-46cf-91a9-3203bccbe9a4",
   "metadata": {},
   "source": [
    "### 1ï¼Œtokenç¼–ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fcd2921-ca7d-43c6-b49b-e4e6a65ed37d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:36:58.900276181Z",
     "start_time": "2023-12-03T14:36:58.858191131Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "#å°†messagesç¼–ç æˆ token, åŒæ—¶è¿”å›labels, è¯¥å‡½æ•°é€‚ç”¨äºå¤šè½®å¯¹è¯æ•°æ®\n",
    "#æ³¨æ„baichuan-13bé€šè¿‡æ’å…¥tokenizer.user_token_idå’Œtokenizer.assistant_token_id æ¥åŒºåˆ†ç”¨æˆ·å’Œæœºå™¨äººä¼šè¯å†…å®¹\n",
    "\n",
    "# reference@ model._build_chat_input?\n",
    "def build_chat_input(messages, model=model,\n",
    "                     tokenizer=tokenizer, \n",
    "                     max_new_tokens: int=0):\n",
    "    max_new_tokens = max_new_tokens or model.generation_config.max_new_tokens\n",
    "    max_input_tokens = model.config.model_max_length - max_new_tokens\n",
    "    max_input_tokens = max(model.config.model_max_length // 2, max_input_tokens)\n",
    "    \n",
    "    total_input, round_input, total_label, round_label = [], [], [], []\n",
    "    \n",
    "    for i, message in enumerate(messages[::-1]):\n",
    "        content_tokens = tokenizer.encode(message['content'])\n",
    "        if message['role'] == 'user':\n",
    "            round_input = [model.generation_config.user_token_id] + content_tokens + round_input\n",
    "            round_label = [-100]+[-100 for _ in content_tokens]+ round_label\n",
    "            \n",
    "            if total_input and len(total_input) + len(round_input) > max_input_tokens:\n",
    "                break\n",
    "            else:\n",
    "                total_input = round_input + total_input\n",
    "                total_label = round_label + total_label\n",
    "                if len(total_input) >= max_input_tokens:\n",
    "                    break\n",
    "                else:\n",
    "                    round_input = []\n",
    "                    round_label = []\n",
    "                    \n",
    "        elif message['role'] == 'assistant':\n",
    "            round_input = [\n",
    "                model.generation_config.assistant_token_id\n",
    "            ] + content_tokens + [\n",
    "                model.generation_config.eos_token_id\n",
    "            ] + round_input\n",
    "\n",
    "            round_label = [\n",
    "                -100\n",
    "            ] + content_tokens + [\n",
    "                model.generation_config.eos_token_id\n",
    "            ]+ round_label\n",
    "        else:\n",
    "            raise ValueError(f\"message role not supported yet: {message['role']}\")\n",
    "            \n",
    "    total_input = total_input[-max_input_tokens:]  # truncate left\n",
    "    total_label = total_label[-max_input_tokens:]\n",
    "    \n",
    "    total_input.append(model.generation_config.assistant_token_id)\n",
    "    total_label.append(-100)\n",
    "    \n",
    "    return total_input,total_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9290-53a7-4ae3-a1ca-d972512c72cb",
   "metadata": {},
   "source": [
    "### 2ï¼Œåšæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9a76500-01db-4c34-b2ba-6bc1c56233d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:00.811772722Z",
     "start_time": "2023-12-03T14:38:00.764611992Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,df,\n",
    "                 prefix=prefix\n",
    "                ):\n",
    "        self.df = df \n",
    "        self.prefix=prefix\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def get_samples(self,index):\n",
    "        samples = []\n",
    "        d = dict(self.df.iloc[index])\n",
    "        samples.append(d)\n",
    "        return samples\n",
    "    \n",
    "    def get_messages(self,index):\n",
    "        samples = self.get_samples(index)\n",
    "        messages = []\n",
    "        for i,d in enumerate(samples):\n",
    "            if i==0:\n",
    "                messages.append({'role':'user','content':self.prefix+d['INPUT']+' -> '})\n",
    "            else:\n",
    "                messages.append({'role':'user','content':d['INPUT']+' -> '})\n",
    "            \n",
    "            messages.append({'role':'assistant','content':d['tag']})\n",
    "        return messages\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        messages = self.get_messages(index)\n",
    "        input_ids, labels = build_chat_input(messages)\n",
    "        return {'input_ids':input_ids,'labels':labels}\n",
    "\n",
    "    def show_sample(self,index):\n",
    "        samples = self.get_samples(index)\n",
    "        print(samples)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cc385bf-0103-4720-a4bd-42744aa76f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:01.709961955Z",
     "start_time": "2023-12-03T14:38:01.704423479Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = MyDataset(dftrain)\n",
    "ds_val = MyDataset(dfval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f19fe484-bc8b-44dd-8b26-d1a8714c0990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:02.735937360Z",
     "start_time": "2023-12-03T14:38:02.694356804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'INPUT': 'Sell USDCNY 5M', 'è´­æ±‡ç»“æ±‡': None, 'tag': 'nan'}]\n"
     ]
    }
   ],
   "source": [
    "ds_train.show_sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fcb4c4b-a4c7-4439-92e0-61f2148c13d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:04.410216256Z",
     "start_time": "2023-12-03T14:38:04.405156365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [195, 26553, 13813, 11536, 3226, 65, 24450, 92569, 4762, 24734, 87639, 92974, 93362, 92600, 92646, 93362, 1833, 92323, 2410, 92974, 93362, 92347, 26549, 32405, 65, 92646, 93362, 92347, 27482, 32405, 66, 1786, 93710, 92846, 4762, 24734, 1667, 3618, 10747, 92974, 93362, 92600, 92646, 93362, 65, 92948, 10078, 19281, 66, 5, 43707, 2204, 84529, 65, 5169, 9426, 2639, 92345, 5, 5, 84, 5, 4762, 70, 7413, 5673, 92974, 93362, 5, 12785, 70, 92974, 93362, 5, 5, 4762, 70, 32574, 92646, 93362, 5, 12785, 70, 92646, 93362, 5, 5, 4762, 70, 32574, 92974, 93362, 5, 12785, 70, 92974, 93362, 5, 5, 4762, 70, 93296, 5673, 92932, 5673, 5, 12785, 70, 19281, 5, 84, 5, 5, 41639, 1558, 4139, 4762, 65, 11065, 12785, 92646, 93362, 69, 92974, 93362, 92600, 19281, 65, 2173, 4193, 2834, 2185, 12889, 2079, 70, 5, 92340, 1547, 2684, 8741, 13765, 92311, 92358, 92353, 92311, 63, 92574, 92311, 196, 19281, 2, 196], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19281, 2, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80b2ee-7d70-41a7-8cf7-2bb5fdfff151",
   "metadata": {},
   "source": [
    "### 3ï¼Œåˆ›å»ºç®¡é“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e43aaccc-863e-4794-8cb5-683d5a727ada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:32.466066986Z",
     "start_time": "2023-12-03T14:38:32.457460101Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_collator(examples: list):\n",
    "    len_ids = [len(example[\"input_ids\"]) for example in examples]\n",
    "    longest = max(len_ids) #ä¹‹åæŒ‰ç…§batchä¸­æœ€é•¿çš„input_idsè¿›è¡Œpadding\n",
    "    \n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for length, example in sorted(zip(len_ids, examples), key=lambda x: -x[0]):\n",
    "        ids = example[\"input_ids\"]\n",
    "        labs = example[\"labels\"]\n",
    "        \n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - length)\n",
    "        labs = labs + [-100] * (longest - length)\n",
    "        \n",
    "        input_ids.append(torch.LongTensor(ids))\n",
    "        labels_list.append(torch.LongTensor(labs))\n",
    "          \n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "698aa807-e05a-42a8-b392-747982363075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:33.719937897Z",
     "start_time": "2023-12-03T14:38:33.676411737Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "dl_train = torch.utils.data.DataLoader(ds_train,num_workers=2,batch_size=4,\n",
    "                                       pin_memory=True,shuffle=True,\n",
    "                                       collate_fn = data_collator)\n",
    "\n",
    "dl_val = torch.utils.data.DataLoader(ds_val,num_workers=2,batch_size=4,\n",
    "                                    pin_memory=True,shuffle=False,\n",
    "                                     collate_fn = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db52d785-10de-4465-8fc2-9eaccaf3387d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:35.144120796Z",
     "start_time": "2023-12-03T14:38:35.050405430Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35002105-b8e8-4d48-a8a2-cbd4ceaa53ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:37.831776784Z",
     "start_time": "2023-12-03T14:38:37.080635945Z"
    }
   },
   "outputs": [],
   "source": [
    "#è¯•è·‘ä¸€ä¸ªbatch\n",
    "out = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c721189c-6443-4e10-b923-327a0beba2fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:40.744123307Z",
     "start_time": "2023-12-03T14:38:40.737410731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.4941, dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cac7d4a-a37b-4c30-8a7d-be552d8e67ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:42.056127293Z",
     "start_time": "2023-12-03T14:38:42.049291641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2eb0f563-61b0-4576-a3b1-08aa4b62e52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:43.828631640Z",
     "start_time": "2023-12-03T14:38:43.814943906Z"
    }
   },
   "outputs": [],
   "source": [
    "#é‡‡æ ·300ä¸ªbatchä½œä¸ºä¸€ä¸ªepochï¼Œä¾¿äºè¾ƒå¿«éªŒè¯\n",
    "dl_train.size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd7e26-985a-4392-ba0d-acb254064677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aa1deea-3fc4-4051-80e9-89de248d3107",
   "metadata": {},
   "source": [
    "## äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644837f5-bbc4-4f4f-a5f0-1f763e36f373",
   "metadata": {},
   "source": [
    "ä¸‹é¢æˆ‘ä»¬å°†ä½¿ç”¨QLoRA(å®é™…ä¸Šç”¨çš„æ˜¯é‡åŒ–çš„AdaLoRAï¼‰ç®—æ³•æ¥å¾®è°ƒBaichuan-13bæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7dbbece-2e50-4166-9e43-ea35d04af856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:46.984400800Z",
     "start_time": "2023-12-03T14:38:46.957552274Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, TaskType\n",
    "model.supports_gradient_checkpointing = True  #\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc66d6a4-77ff-490c-a3c0-913a9316eb13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:48.228465112Z",
     "start_time": "2023-12-03T14:38:48.223045143Z"
    }
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb \n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    æ‰¾å‡ºæ‰€æœ‰å…¨è¿æ¥å±‚ï¼Œä¸ºæ‰€æœ‰å…¨è¿æ¥æ·»åŠ adapter\n",
    "    \"\"\"\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "587a53eb-ba75-40a6-83e6-1ad2684738f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:38:53.905437905Z",
     "start_time": "2023-12-03T14:38:53.857409432Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training \n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffbd01ad-2667-4c38-8ce7-62ce293e759e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:48:04.450196355Z",
     "start_time": "2023-12-03T14:48:04.431834901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up_proj', 'down_proj', 'gate_proj', 'W_pack', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "lora_modules = find_all_linear_names(model)\n",
    "print(lora_modules) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5b68e33-3c18-4044-9554-c453b17d4fef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:48:06.515709862Z",
     "start_time": "2023-12-03T14:48:06.282985097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26,838,912 || all params: 7,532,812,320 || trainable%: 0.35629338499170254\n"
     ]
    }
   ],
   "source": [
    "from peft import AdaLoraConfig\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules= lora_modules\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "372d96e5-f5c4-4a21-a09b-825df8563399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:48:08.770526660Z",
     "start_time": "2023-12-03T14:48:08.636569566Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mpeft_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1003\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m    992\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforward in MPTForCausalLM does not support inputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    993\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(\n\u001B[1;32m    994\u001B[0m             input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    995\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1000\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1001\u001B[0m         )\n\u001B[0;32m-> 1003\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1014\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/adalora/model.py:237\u001B[0m, in \u001B[0;36mAdaLoraModel.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 237\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(outputs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    240\u001B[0m         \u001B[38;5;66;03m# Calculate the orthogonal regularization\u001B[39;00m\n\u001B[1;32m    241\u001B[0m         orth_reg_weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainable_adapter_name]\u001B[38;5;241m.\u001B[39morth_reg_weight\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:684\u001B[0m, in \u001B[0;36mBaichuanForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    681\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    683\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 684\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    696\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    697\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:453\u001B[0m, in \u001B[0;36mBaichuanModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    449\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m module(\u001B[38;5;241m*\u001B[39minputs, output_attentions, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m custom_forward\n\u001B[0;32m--> 453\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckpoint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckpoint\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_custom_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder_layer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[1;32m    462\u001B[0m         hidden_states,\n\u001B[1;32m    463\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    467\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    468\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dynamo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:451\u001B[0m, in \u001B[0;36mcheckpoint\u001B[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001B[0m\n\u001B[1;32m    446\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m context_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m noop_context_fn \u001B[38;5;129;01mor\u001B[39;00m debug \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    448\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing `context_fn` or `debug` is only supported when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    449\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_reentrant=False.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    450\u001B[0m         )\n\u001B[0;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mCheckpointFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreserve\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    453\u001B[0m     gen \u001B[38;5;241m=\u001B[39m _checkpoint_without_reentrant_generator(\n\u001B[1;32m    454\u001B[0m         function, preserve, context_fn, determinism_check, debug, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    455\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:539\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    538\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    547\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:230\u001B[0m, in \u001B[0;36mCheckpointFunction.forward\u001B[0;34m(ctx, run_function, preserve_rng_state, *args)\u001B[0m\n\u001B[1;32m    227\u001B[0m ctx\u001B[38;5;241m.\u001B[39msave_for_backward(\u001B[38;5;241m*\u001B[39mtensor_inputs)\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 230\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mrun_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:449\u001B[0m, in \u001B[0;36mBaichuanModel.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001B[0;34m(*inputs)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcustom_forward\u001B[39m(\u001B[38;5;241m*\u001B[39minputs):\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;66;03m# None for past_key_value\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:273\u001B[0m, in \u001B[0;36mDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    270\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m hidden_states, self_attn_weights, present_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:205\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    195\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    196\u001B[0m         hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    201\u001B[0m         use_cache: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    202\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, Optional[torch\u001B[38;5;241m.\u001B[39mTensor], Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]]]:\n\u001B[1;32m    203\u001B[0m     bsz, q_len, _ \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m--> 205\u001B[0m     proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW_pack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     proj \u001B[38;5;241m=\u001B[39m proj\u001B[38;5;241m.\u001B[39munflatten(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size))\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    207\u001B[0m     query_states \u001B[38;5;241m=\u001B[39m proj[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mview(bsz, q_len, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/adalora/bnb.py:144\u001B[0m, in \u001B[0;36mSVDLinear4bit.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m requires_conversion:\n\u001B[1;32m    143\u001B[0m     expected_dtype \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m--> 144\u001B[0m     compute_dtype \u001B[38;5;241m=\u001B[39m \u001B[43mlora_A\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m compute_dtype:\n\u001B[1;32m    146\u001B[0m         x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(compute_dtype)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Parameter' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "out = peft_model.forward(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b1c036a-c466-45a8-816e-e93da6810c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:40:02.420414706Z",
     "start_time": "2023-12-03T14:40:02.413233926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.4941, dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddcd08-851b-42e0-a1ac-db530e728aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad2444d-2c9c-4048-958c-ae48d9590dfd",
   "metadata": {},
   "source": [
    "## ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9317cd85-b886-4153-af53-42c83921525a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:40:08.908075216Z",
     "start_time": "2023-12-03T14:40:08.699399933Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net.forward(**batch)[0]\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "#ä»…ä»…ä¿å­˜QLoraå¯è®­ç»ƒå‚æ•°\n",
    "def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint'):\n",
    "    import os\n",
    "    self.net.load_state_dict(\n",
    "        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a96d4-0fef-419a-a345-f5a5342ab8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbed7bbe-10a7-4132-8d02-ead60b4d59ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:40:16.838797021Z",
     "start_time": "2023-12-03T14:40:16.836061623Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = bnb.optim.adamw.AdamW(peft_model.parameters(),\n",
    "                                  lr=6e-05,is_paged=True)  #'paged_adamw'\n",
    "keras_model = KerasModel(peft_model,loss_fn =None,\n",
    "        optimizer=optimizer) \n",
    "ckpt_path = 'baichuan13b_waimai'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "131ba617-d6e6-4bf7-9259-daaa113ada8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:40:19.917864914Z",
     "start_time": "2023-12-03T14:40:19.465563779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;31m<<<<<< âš¡ï¸ cuda is used >>>>>>\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 600x400 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<style>\n    /* background: */\n    progress::-webkit-progress-bar {background-color: #CDCDCD; width: 100%;}\n    progress {background-color: #CDCDCD;}\n\n    /* value: */\n    progress::-webkit-progress-value {background-color: #00BFFF  !important;}\n    progress::-moz-progress-bar {background-color: #00BFFF  !important;}\n    progress {color: #00BFFF ;}\n\n    /* optional */\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #000000;\n    }\n</style>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n      <br>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# keras_model.load_ckpt(ckpt_path) #æ”¯æŒåŠ è½½å¾®è°ƒåçš„æƒé‡ç»§ç»­è®­ç»ƒ(æ–­ç‚¹ç»­è®­)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mkeras_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdl_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                \u001B[49m\u001B[43mval_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdl_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_loss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m               \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/MLE/tuning/torchkeras/torchkeras/kerasmodel.py:213\u001B[0m, in \u001B[0;36mKerasModel.fit\u001B[0;34m(self, train_data, val_data, epochs, ckpt_path, patience, monitor, mode, callbacks, plot, wandb, quiet, mixed_precision, cpu, gradient_accumulation_steps)\u001B[0m\n\u001B[1;32m    211\u001B[0m train_epoch_runner \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mEpochRunner(train_step_runner,should_quiet)\n\u001B[1;32m    212\u001B[0m train_metrics \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m:epoch}\n\u001B[0;32m--> 213\u001B[0m train_metrics\u001B[38;5;241m.\u001B[39mupdate(\u001B[43mtrain_epoch_runner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, metric \u001B[38;5;129;01min\u001B[39;00m train_metrics\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistory[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistory\u001B[38;5;241m.\u001B[39mget(name, []) \u001B[38;5;241m+\u001B[39m [metric]\n",
      "File \u001B[0;32m~/Projects/MLE/tuning/torchkeras/torchkeras/kerasmodel.py:77\u001B[0m, in \u001B[0;36mEpochRunner.__call__\u001B[0;34m(self, dataloader)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, batch \u001B[38;5;129;01min\u001B[39;00m loop: \n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet):\n\u001B[0;32m---> 77\u001B[0m         step_losses,step_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msteprunner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m   \n\u001B[1;32m     78\u001B[0m         step_log \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(step_losses,\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstep_metrics)\n\u001B[1;32m     79\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k,v \u001B[38;5;129;01min\u001B[39;00m step_losses\u001B[38;5;241m.\u001B[39mitems():\n",
      "Cell \u001B[0;32mIn[66], line 20\u001B[0m, in \u001B[0;36mStepRunner.__call__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[1;32m     17\u001B[0m     \n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m#loss\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mautocast():\n\u001B[0;32m---> 20\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m#backward()\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstage\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1003\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m    992\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforward in MPTForCausalLM does not support inputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    993\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(\n\u001B[1;32m    994\u001B[0m             input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    995\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1000\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1001\u001B[0m         )\n\u001B[0;32m-> 1003\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1014\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/adalora/model.py:237\u001B[0m, in \u001B[0;36mAdaLoraModel.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 237\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(outputs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    240\u001B[0m         \u001B[38;5;66;03m# Calculate the orthogonal regularization\u001B[39;00m\n\u001B[1;32m    241\u001B[0m         orth_reg_weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainable_adapter_name]\u001B[38;5;241m.\u001B[39morth_reg_weight\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:684\u001B[0m, in \u001B[0;36mBaichuanForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    681\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    683\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 684\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    696\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    697\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:453\u001B[0m, in \u001B[0;36mBaichuanModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    449\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m module(\u001B[38;5;241m*\u001B[39minputs, output_attentions, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m custom_forward\n\u001B[0;32m--> 453\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckpoint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckpoint\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_custom_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder_layer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[1;32m    462\u001B[0m         hidden_states,\n\u001B[1;32m    463\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    467\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    468\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dynamo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:451\u001B[0m, in \u001B[0;36mcheckpoint\u001B[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001B[0m\n\u001B[1;32m    446\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m context_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m noop_context_fn \u001B[38;5;129;01mor\u001B[39;00m debug \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    448\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing `context_fn` or `debug` is only supported when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    449\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_reentrant=False.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    450\u001B[0m         )\n\u001B[0;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mCheckpointFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreserve\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    453\u001B[0m     gen \u001B[38;5;241m=\u001B[39m _checkpoint_without_reentrant_generator(\n\u001B[1;32m    454\u001B[0m         function, preserve, context_fn, determinism_check, debug, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    455\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:539\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    538\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    547\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:230\u001B[0m, in \u001B[0;36mCheckpointFunction.forward\u001B[0;34m(ctx, run_function, preserve_rng_state, *args)\u001B[0m\n\u001B[1;32m    227\u001B[0m ctx\u001B[38;5;241m.\u001B[39msave_for_backward(\u001B[38;5;241m*\u001B[39mtensor_inputs)\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 230\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mrun_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:449\u001B[0m, in \u001B[0;36mBaichuanModel.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001B[0;34m(*inputs)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcustom_forward\u001B[39m(\u001B[38;5;241m*\u001B[39minputs):\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;66;03m# None for past_key_value\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:273\u001B[0m, in \u001B[0;36mDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    270\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m hidden_states, self_attn_weights, present_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py:205\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    195\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    196\u001B[0m         hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    201\u001B[0m         use_cache: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    202\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, Optional[torch\u001B[38;5;241m.\u001B[39mTensor], Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]]]:\n\u001B[1;32m    203\u001B[0m     bsz, q_len, _ \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m--> 205\u001B[0m     proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW_pack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     proj \u001B[38;5;241m=\u001B[39m proj\u001B[38;5;241m.\u001B[39munflatten(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size))\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    207\u001B[0m     query_states \u001B[38;5;241m=\u001B[39m proj[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mview(bsz, q_len, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/adalora/bnb.py:144\u001B[0m, in \u001B[0;36mSVDLinear4bit.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m requires_conversion:\n\u001B[1;32m    143\u001B[0m     expected_dtype \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m--> 144\u001B[0m     compute_dtype \u001B[38;5;241m=\u001B[39m \u001B[43mlora_A\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m compute_dtype:\n\u001B[1;32m    146\u001B[0m         x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(compute_dtype)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Parameter' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# keras_model.load_ckpt(ckpt_path) #æ”¯æŒåŠ è½½å¾®è°ƒåçš„æƒé‡ç»§ç»­è®­ç»ƒ(æ–­ç‚¹ç»­è®­)\n",
    "keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=100,patience=10,\n",
    "                monitor='val_loss',mode='min',\n",
    "                ckpt_path = ckpt_path\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108976a7-17cc-411e-8532-d781d65d1f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a55a90-f980-4dbd-b8f8-6c0e0c8d5a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455498a9-3613-4efa-b7d4-6035314c8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49763c6a-6ee5-448c-b873-48825a6fba67",
   "metadata": {},
   "source": [
    "## å››ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98c4e5-dbb0-456e-bc4b-1c8dfccbd499",
   "metadata": {},
   "source": [
    "ä¸ºå‡å°‘GPUå‹åŠ›ï¼Œæ­¤å¤„å¯é‡å¯kernelé‡Šæ”¾æ˜¾å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee686c58-42e2-463a-a429-3fe16dee5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142a917e-9db0-4d5a-8352-cb250561f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-19 23:21:49,363] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c6bf03a09d40838ad3c888d990e49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, AutoModel, BitsAndBytesConfig\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "import torch.nn as nn\n",
    "model_name_or_path ='../baichuan-13b'\n",
    "ckpt_path = 'baichuan13b_waimai'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model_old = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8b409f-f100-48c5-9794-62cb5bdf19d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "#å¯èƒ½éœ€è¦5åˆ†é’Ÿå·¦å³\n",
    "peft_model = PeftModel.from_pretrained(model_old, ckpt_path)\n",
    "model_new = peft_model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c060b4a-7731-4ec7-8699-9f47088f4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils import GenerationConfig\n",
    "model_new.generation_config = GenerationConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83a654-1d18-47e7-bbf1-d67ca351813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524a3d55-5daf-4ff1-8fb9-e3dd60d65f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¹”æˆˆé‡Œå³°ã€‚ä¸–ç•Œç¬¬äºŒé«˜å³°â€”â€”â€”ä¹”æˆˆé‡Œå³°è¥¿æ–¹ç™»å±±è€…ç§°å…¶ä¸ºk2å³°ï¼Œæµ·æ‹”é«˜åº¦æ˜¯8611ç±³ï¼Œä½äºå–€å–‡æ˜†ä»‘å±±è„‰çš„ä¸­å·´è¾¹å¢ƒä¸Š\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\",\n",
    "                 \"content\": \"ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±å³°æ˜¯ä»€ä¹ˆï¼Ÿ\"})\n",
    "response = model_new.chat(tokenizer,messages=messages,stream=True)\n",
    "for res in response:\n",
    "    print(res)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce165cd8-987d-47be-95ac-83cf7ef479b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'baichuan-13b-waimai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9214d354-811b-4e47-96ac-682ed5e68ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_path)\n",
    "model_new.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7950603-e844-4d60-990d-f86245fbaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../baichuan-13b/*.py  baichuan-13b-waimai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91c249-3402-489b-ae8e-5554a5082562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81582c3e-28d3-4847-a84c-dc1b53aa0839",
   "metadata": {},
   "source": [
    "## äº”ï¼Œä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c57c2f-0e1a-4e77-b4e4-fefa210b1bb1",
   "metadata": {},
   "source": [
    "ä¸ºå‡å°‘GPUå‹åŠ›ï¼Œæ­¤å¤„å¯å†æ¬¡é‡å¯kernelé‡Šæ”¾æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bec4773-61f0-4aa2-9ac2-d2e1c295b2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc27e2a0c9414f79a4da3b8466dce922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, BitsAndBytesConfig\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_name_or_path = 'baichuan-13b-waimai'\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "   model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True) \n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e958b920-63cd-4e1a-bd28-d36e7c281901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¹”æˆˆé‡Œå³°ã€‚ä¸–ç•Œç¬¬äºŒé«˜å³°â€”â€”â€”ä¹”æˆˆé‡Œå³°\n",
      "æµ·æ‹”é«˜åº¦ï¼š8610ç±³\n",
      "åæ ‡çº¬åº¦ï¼š35Â°49â€²15â€²â€²n,76Â°21â€²24â€²â€²e\n",
      "åœ°ç†ä½ç½®ï¼šå–€å–‡æ˜†ä»‘å±±è„‰ä¸­å·´è¾¹å¢ƒä¸Š\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\",\n",
    "                 \"content\": \"ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±å³°æ˜¯ä»€ä¹ˆï¼Ÿ\"})\n",
    "response = model.chat(tokenizer,messages=messages,stream=True)\n",
    "for res in response:\n",
    "    print(res)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d6988-4cfd-46ba-8a5d-f11e5d183483",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹å¾®è°ƒåçš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e7ad1b-9831-4b11-9ef8-a848a49c9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datasets \n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a77d28-11ab-423c-8856-c4027d2e9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"å¤–å–è¯„è®ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡:\n",
    "ä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹:\n",
    "\n",
    "å‘³é“çœŸä¸é”™ -> å¥½è¯„\n",
    "å¤ªè¾£äº†ï¼Œåƒä¸ä¸‹éƒ½  -> å·®è¯„\n",
    "\n",
    "è¯·å¯¹ä¸‹è¿°è¯„è®ºè¿›è¡Œåˆ†ç±»ã€‚è¿”å›'å¥½è¯„'æˆ–è€…'å·®è¯„'ã€‚\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(text):\n",
    "    return prefix+text+' -> '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "465724e3-70ef-4500-9154-eaf626f7701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½è¯„\n"
     ]
    }
   ],
   "source": [
    "messages  = [{\"role\": \"user\", \"content\": get_prompt('å‘³é“ä¸é”™ï¼Œä¸‹æ¬¡å†æ¥')}]\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4130efa-795f-4d2a-ac96-378040ebc540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"å¤–å–è¯„è®ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡:\\nä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹:\\n\\nå‘³é“çœŸä¸é”™ -> å¥½è¯„\\nå¤ªè¾£äº†ï¼Œåƒä¸ä¸‹éƒ½  -> å·®è¯„\\n\\nè¯·å¯¹ä¸‹è¿°è¯„è®ºè¿›è¡Œåˆ†ç±»ã€‚è¿”å›'å¥½è¯„'æˆ–è€…'å·®è¯„'ã€‚\\nå‘³é“ä¸é”™ï¼Œä¸‹æ¬¡å†æ¥ -> \"}, {'role': 'assistant', 'content': 'å¥½è¯„'}]\n"
     ]
    }
   ],
   "source": [
    "messages = messages+[{\"role\": \"assistant\", \"content\": response}]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50bad51c-2f1d-4af2-bb63-42cf845bf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message(prompt,response):\n",
    "    return [{\"role\": \"user\", \"content\": f'{prompt} -> '},\n",
    "            {\"role\": \"assistant\", \"content\": response}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89f4cd73-b6a9-47fe-aae8-b8d5b66a2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(get_message('å¤ªè´µäº†','å·®è¯„'))\n",
    "messages.extend(get_message('éå¸¸å¿«ï¼Œå‘³é“å¥½','å¥½è¯„'))\n",
    "messages.extend(get_message('è¿™ä¹ˆå’¸ï¼ŒçœŸçš„æ˜¯é†‰äº†','å·®è¯„'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8c57fea-721e-45d3-81b5-3b5500d21b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,temperature=0.01):\n",
    "    model.generation_config.temperature=temperature\n",
    "    response = model.chat(tokenizer, \n",
    "                          messages = messages+[{'role':'user','content':f'{text} -> '}])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aed22302-e528-4563-bd07-7c5fcd8c0dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å¥½è¯„'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('æ­»é¬¼ï¼Œæ€ä¹ˆå¼„å¾—è¿™ä¹ˆå¥½åƒå‘¢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db357a8c-83d4-4de4-83b9-f1cb0e284846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ•°æ®é›†åŠ è½½\n",
    "dftest = pd.read_parquet('../data/dftest.parquet')[['text','label','tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e765ac-21be-475d-9076-32ddcc867df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [10:24<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dftest['pred'] = [predict(text) for text in tqdm(dftest['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdc2832d-3f0d-49b2-93e9-d307ce771f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc =  0.9015\n"
     ]
    }
   ],
   "source": [
    "print('acc = ',len(dftest.query('tag==pred'))/len(dftest))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d9def-fb3b-4bf1-8579-b5e28415082c",
   "metadata": {},
   "source": [
    "å¾®è°ƒåçš„accä¸º0.9015ï¼Œç›¸æ¯”å¾®è°ƒå‰çš„0.8925ï¼Œçº¦æå‡1ä¸ªç™¾åˆ†ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a304e-2e8b-4fe2-bc79-e2ed9bbadd5c",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœåœ¨torchkerasçš„ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥åœ¨é¡¹ç›®ä¸­æäº¤issueã€‚\n",
    "\n",
    "å¦‚æœæƒ³è¦è·å¾—æ›´å¿«çš„åé¦ˆæˆ–è€…ä¸å…¶ä»–torchkerasç”¨æˆ·å°ä¼™ä¼´è¿›è¡Œäº¤æµï¼Œ\n",
    "\n",
    "å¯ä»¥åœ¨å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ã€‚\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
